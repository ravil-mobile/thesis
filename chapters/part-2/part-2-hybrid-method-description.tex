\label{subseq:hybrid-method-description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Nowadays, iterative methods is a common choice for solving sparse systems of linear equations because of their possible fast convergence and high parallel efficiency. However, application of such method always demands preconditioning of ill-conditioned systems to make methods converge to numerical accurate solutions. It can be clearly observed from table \ref{table:grs-matrix-set} that numerical integration of thermo-hydraulic simulations in ATHLET results in solving such ill-conditioned systems  based on estimated condition numbers of matrices form GRS matrix set.\\


As the first step of the study, we tested various preconditioning algorithms together with their tuning parameters, mentioned in table \ref{table:preconditioners}, applied to GRS matrix set. GMRES was chosen as an iterative solver with values of relative and absolute convergence tolerances in the residual norm to be equal to $1E-8$ and $1E-4$, respectively. A coarse grid search was used with maximum 3 values for each tuning parameter starting from the default towards more accurate values in order to refine settings of each preconditioning algorithm. In spite of vast amount of testing, results showed that none of them could lead to convergence of the entire matrix set.\\


One can assume that a finer grid search can result in finding a suitable preconditioning algorithm with such settings that can lead to convergence of GMRES solver for all matrices from the set. However, it is important to point out the matrices were generated by running the most common GRS thermo-hydraulic test-scenarios and saving them somewhere in the middle of the time integration process. Hence, there is no guarantee that the settings found in such a way can always lead to convergence of GMRES solver in all time steps of any thermo-hydraulic simulation. Therefore, iterative methods may not satisfy \textit{robustness} criteria stated in chapter \ref{chapter:problem-statment} as a non-functional requirement to the time integration solver used in ATHLET.\\


Taking into account the above reasoning, we have come to the conclusion that sparse direct methods is the best choice for out problem, in spite of the limited tree-task parallelism described in subsection \ref{subseq:direct-parallel-aspects}, because the methods stably result in numerical accurate solutions even in case of ill-conditioned linear systems. Hence, the next objective of the study is to find a suitable sparse direct method and its implementation, and adapt it for HW1 compute-cluster environment in terms of efficient parallel execution. \\



%\todo{SDS: start - read comments}
%We can conclude, from the analysis above, the method has inherently bad scaling behavior and it is quite sensitive to a matrix structure. We will see later that it is almost impossible to predict the saturation point i.e. a point after which performance either drops or stays at the same level. We assume that scaling becomes better with growth of a matrix size. However, we cannot expect such behavior for small and medium systems.\\  


%Secondly, we can see the algorithm requires many pre-processing steps to be done before numerical factorization phase. All these steps must run in parallel and be highly scalable. Apart from performance constrains of the steps, they must lead to wide and well balanced elimination trees which becomes crucial during the numerical phase.\\


%Lastly, the algorithm can fail due to incorrect  working space prediction. As a results, factorization has to be restarted with some modification of input solver parameters.\\
%\todo{SDS: end}



%\textbf{Preconditioners start:}
%At the first step, some methods, table BRA, were tried with tier default settings. The algorithms were chosen because of their availability in PETSc and to run in parallel.\\




% native PETSc preconditioners as well as some external ones

%Table \todo{Table with comparisons of different preconditioning for our test cases}

%Table [] shows results of different preconditioning algorithms applied to our test case. It can be seen that some algorithms failed even after tuning. \\

%It interesting to notice that \citeauthor{wsmp} came to approximately the same results working on their set of matrices in their work \cite{wsmp}. They observed that preconditioned iterative solvers worked efficiently only for 2 out 5 cases in contrast to direct sparse solvers.\\

%We can summarize that it is vital to perform careful parameter tuning of any preconditioning algorithms combining results from [table] and \cite{wsmp}. In general the  search can take a considerable amount of time. Moreover, it becomes impractical for time integration problems where topology of an underlying problem and, as the results, the computational mesh, discretization, Jacobian matrix can be changed over time of a simulation. It is obvious that parameters chosen for a particular time step can become not optimal for consecutive steps and, at the end, it can lead to divergence. If divergence happens at any time step the entire time integration algorithm fails and the simulation has to be restarted with different preconditioning parameters or with a different preconditioning algorithm.\\

%By and large we come to the conclusion that preconditioned iterative solvers are not robust and thus cannot fully fulfill requirements listed in section \ref{chapter:solver configuration}.\\

% maybe above we have to mention inderect memory access and its effect on performance


%\textbf{Preconditioners end:}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55



% show motivation to use sparce direct methods: show a plot whith MUMPS and 100 interation of GMRES
% show all phases

%We have observed almost all available methods and we could see that none of them can fully cover all our requirements at once, namely:

%\begin{itemize}
%	\item robustness
%	\item numerical stability
%	\item parallel efficiency
%	\item open source licenses
%\end{itemize}


%The analysis from sections \ref{subseq:iterative methods} and \ref{subseq:iterative methods} shows that iterative methods scale much better in contrast to sparse direct ones. However, they are only efficient in case of very well preconditioned systems. We showed in section \ref{subseq:iterative methods} that search of preconditioning parameters usually takes lots of time and efforts. Additionally, we cannot guarantee that the settings fond for our GRS matrix set will always work well in subsequent steps of time integration or for other different simulations.\\


%Sparse direct methods do not suffer from problem. They always produce \todo{grammar}the right solution. The methods can only fail in case of underestimation of the working space due to numerical pivoting during the numerical factorization phase. In order to cope with that problems, some implementations of direct sparse methods provide two options to the user, namely: to increase predicted working space by some factor e.g. 2, 3, 4, etc. or to lower constrains of numerical pivoting which allows small numerical values to stay on the diagonal.\\


%The drawback of the second option is that it can lead to out-of-core execution with using the secondary memory which makes numerical factorization significantly slow. While the second option has lower chance of out-of-core factorization it can lead to a numerically inaccurate solution. \\


%\todo{to fullfill robustness creteria} After many considerations we decided to stick to the sparse direct solvers because \textbf{robustness} criteria had the highest priority in our case. To circumvent problems mentioned above, we proposed a so-called hybrid solver, in spite of the fact that the definition of \textit{hybrid linear solvers} had already been used in scientific computing literature in a slightly different way \cite{shylu-hybrid-solver}. \textbf{\textit{The idea is to switch off numerical pivoting (or significantly lower the constrains) of sparse direct solvers and use the resultant $LU$ decomposition as a preconditioner for an iterative method, for example GMRES}}.\\

%\begin{lstlisting}[language=python, caption={A pseudo-code of the Hybrid approach}, frame=single]
%# compute LU decomposition with a sparse direct solver
%LU = SparseDirectSolver(matrix=A, pivoting="switch_off")

%# compute inverse of A using backward-forward substitutions
%# sovle: LU * A_inv = I
%A_inv = ComputeInverse(decomposition=LU)

%# apply a Krylov method to a preconditioned system 
%# i.e A_inv * A * x = A_inv * b
%GMRES(matrix=A, rhs=b, preconditione=A_inv)
%\end{lstlisting}

%\todo{write which solver we used}
%According to our primary tests, the hybrid approach showed us that it required from 1 to 5 iterations of the GMRES solver on average to converge to a desired residual.\\


%The main problem of our approach is parallel efficiency because sparse $LU$ decomposition takes the most of computational time. We discussed reasons of possible bad strong scaling behavior of sparse direct solvers in section \ref{subseq:sparse methods}. We could see, in case of the multifrontal method, these methods consist of multiple steps and implementation of each step has its strong effect on parallel performance. We also mentioned that the main source of performance improvement is data parallelism and it can be achieved in many different ways. Hence, performance of the same method can vary form library to library.\\


%In the next section we are going to investigate all available open-source implementations of sparse direct solvers, compare their efficiency and choose one of them. At the beginning, we will only consider libraries that have their direct interface to PETSc \cite{petsc-web-page}. PETSc is a scientific numerical library that contains various algorithms and methods, especially the Krylov methods. It is highly efficient in parallel and provides numerous interfaces to other libraries such as MUMPS, SuperLU, Hypre, PaStiX, ViennaCL and etc.\\ 


%\todo{proofreading}
%The subsequent sections will be dedicated to tuning and optimization of a specific library with the aim to reduce execution time. \\