%\subsection{Choice of Fill Reducing Reordering}
\subsection{Fill Reducing Reorderings}
\label{subseq:fill-in-reordering}

Fill reducing reordering is one of the first and the most important steps of sparse matrix factorization. As the name suggests, the step aims to reduce fill-in of $L$ and $U$ factors. However, it may have a strong and indirect impact on the elimination/assembly tree structure. As we discussed in subsection \ref{subseq:direct-parallel-aspects}, the structure defines tree-task parallelism as well as sizes of frontal matrices and, therefore, performance of the method.\\


\acrshort{mumps} provides various algorithms for fill reducing reordering, as it was mentioned above. A detailed study and comparison of different methods were done by \citeauthor{guermouche2003memory}, in work \cite{guermouche2003memory}, for sequential execution of the analysis phase. \citeauthor{guermouche2003memory} noticed that  trees generated by METIS and SCOTCH were rather wide (because of the global partitioning performed at the top), while the trees generated by AMD, AMF and PORD tend to be deeper. In addition, they observed two important things. Firstly, they noticed that both SCOTCH and METIS generated much better balanced trees in contrast to other methods. Secondly, according to their results, SCOTCH and METIS produced trees with bigger frontal matrices in contrast to those trees generated by other reordering techniques, \cite{guermouche2003memory}.\\


In this subsection, we are going to investigate an influence of two different parallel fill reducing reordering algorithms provided by PT-Scotch and ParMETIS libraries on parallel performance of \acrshort{mumps}. The algorithmic difference between the corresponding PT-Scotch and ParMETIS subroutines was mentioned in subsection \ref{subseq:mumps-review}.\\


To perform testing, \acrshort{petsc}, \acrshort{mumps}, PT-Scotch and ParMETIS libraries were downloaded, compiled, configured and link together using their default parameter settings. Tests were carried out using only flat-\acrshort{mpi} mode on \gls{hw1} compute-cluster without any explicit process pinning. The results are shown in figure \ref{fig:mumps-ordering-1} as well as in appendix \ref{app:app-fill-reducing-reodering}.\\


\figpointer{\ref{fig:mumps-ordering-1}}
\begin{figure}[htpb]
\centering
	\begin{tabular}{cc}
		\subfloat[k3-18]{\includegraphics[width=0.48\textwidth]{figures/chapter-2/ordering/k3-18.png}} &
		\subfloat[cube-64]{\includegraphics[width=0.48\textwidth]{figures/chapter-2/ordering/cube-64.png}} \\
		\subfloat[pwr-3d]{\includegraphics[width=0.48\textwidth]{figures/chapter-2/ordering/pwr-3d.png}} &
		\subfloat[k3-2]{\includegraphics[width=0.48\textwidth]{figures/chapter-2/ordering/k3-2.png}} \\
	\end{tabular}
	\caption{An influence of different fill-reducing algorithms on parallel factorization of \acrshort{grs} matrix set}
	\label{fig:mumps-ordering-1}
\end{figure}



According to the results of testing, parallel performance of \acrshort{mumps} can vary significantly and very sensitive to an applied fill-in reducing reordering algorithm. On average, the difference between the algorithms achieves almost \textbf{15\%}. However, in some particular cases, \textit{cube-5} and \textit{pwr-3d}, the difference varies around \textbf{40-55\%}.\\


It is important to mention that both algorithms, PT-Scotch and ParMetis, are based on different heuristic approaches. It is relevant to assume that efficiency of a particular heuristic can be very sensitive to a matrix structure and size. This fact makes it difficult to predict in advance which algorithm is better to use for a specific case.\\


Considering results obtained using \acrshort{grs} matrix set, we can observe that PT-Scotch is the best choice for small- and medium-sized matrices, namely: \textit{cube-5}, \textit{cube-64}, \textit{k3-2} and \textit{pwr-3d} cases. Whereas, PerMetis tends to work better for relatively big systems, such as \textit{cube-645} and \textit{k3-18}, \textit{However, we keep in mind that number of \acrshort{grs} test-cases is not enough to make such conclusion and, therefore, the matrix set must be extended considerably for a future study}.\\


% profiling of cube-5 and pwr-3d with ParMetis
During the testing, we noticed that applications of ParMetis to small systems of equations showed a strong negative effect on parallel performance of \acrshort{mumps}. The results showed that factorization time of \textit{pwr-3d} and \textit{cube-5} matrices grew with the increase of the number of processing units, figure \ref{fig:mumps-ordering-matrices-total-time}.\\


\figpointer{\ref{fig:mumps-ordering-matrices-total-time}}
\begin{figure}[htpb]
\centering
	\begin{tabular}{cc}
		\subfloat[pwr-3d]{\includegraphics[width=0.475\textwidth]{figures/chapter-2/ordering/profiling/total-time-pwr-3d.png}} &
		\subfloat[cube-5]{\includegraphics[width=0.475\textwidth]{figures/chapter-2/ordering/profiling/total-time-cube-5.png}} \\
	\end{tabular}
	\caption{Parallel performance of \acrshort{mumps}-ParMetis configuration in case of parallel factorizations of relatively small matrices}
	\label{fig:mumps-ordering-matrices-total-time}
\end{figure}


A simple profiling showed two important things. Firstly, numerical factorization time and time spent on the analysis phase had approximately the same order in case of sequential execution i.e. 1 \acrshort{mpi} process. Secondly, while numerical factorization time were barely decreasing with increase of the number of processing elements, time spent on the analysis phase significantly grew. Therefore, the slow-down of \acrshort{mumps} in case of these two test-cases mainly came from overheads of the analysis phase.\\


A careful investigation revealed the analysis phase contained several peaks at points where \acrshort{mpi} processor count was equal to a power of two. We assumed the cause could result from either fill reducing reordering or process mapping steps. However, a detailed profiling and tracing of the analysis phase, which are out of the scope of this study, are required in order to give the exact answer. The results of profiling are shown in figure \ref{fig:mumps-ordering-matrices-profiling}.\\


\figpointer{\ref{fig:mumps-ordering-matrices-profiling}}
\begin{figure}[htpb]
\centering
	\begin{tabular}{cc}
		\subfloat[pwr-3d]{\includegraphics[width=0.475\textwidth]{figures/chapter-2/ordering/profiling/profiling-pwr-3d.png}} &
		\subfloat[cube-5]{\includegraphics[width=0.475\textwidth]{figures/chapter-2/ordering/profiling/profiling-cube-5.png}} \\
	\end{tabular}
	\caption{Profiling of \acrshort{mumps}-ParMetis configuration applied to parallel factorizations of relatively small matrices}
	\label{fig:mumps-ordering-matrices-profiling}
\end{figure}


\begin{table}[htpb]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Matrix Name & Ordering  & \textit{n}       & \textit{nnz}      & \textit{nnz} / \textit{n} \\ \hline
cube-5      & PT-Scotch & 9325    & 117897   & 12.6431 \\ \hline
cube-64     & PT-Scotch & 100657  & 1388993  & 13.7993 \\ \hline
cube-645    & ParMetis  & 1000045 & 13906057 & 13.9054 \\ \hline
k3-2        & PT-Scotch & 130101  & 787997   & 6.0568  \\ \hline
k3-18       & ParMetis  & 1155955 & 7204723  & 6.2327  \\ \hline
pwr-3d      & PT-Scotch & 6009    & 32537    & 5.4147  \\ \hline
\end{tabular}
\caption{Assignment of \acrshort{grs} matrices to a specific fill-in reducing algorithm}
\label{table:GRS-ordering-assignment}
\end{table}


In this subsection, we have presented an influence of  two different fill-in reducing algorithms on parallel performance of \acrshort{mumps}. We have observed that a correct choice of an algorithm can lead to a significant improvement in terms of the overall parallel execution time. We have showed there is no a single algorithm that performs the best for all test-cases. At the moment of writing, we have come to the conclusion there is no an indirect metric to predict the best algorithm in advance for a specific system of equations. Sometimes PT-Scotch and ParMetis can result in nearly the same performance as it was, for example, in case of \textit{CurlCurl\_3} and \textit{cant} matrices, see appendix \ref{app:app-fill-reducing-reodering}. Therefore, from time to time, it can be quite difficult to decide which package to use even with available flat-\acrshort{mpi} test results. At the end, we have assigned each test-case to a specific fill reducing reordering method based on results of the conducted experiments and our subjective opinion. The results are summarized it in tables \ref{table:GRS-ordering-assignment} and \ref{table:SuiteSparse-ordering-assignment}. \\ 


\begin{table}[htpb]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Matrix Name & Ordering  & \textit{n}       & \textit{nnz}      & \textit{nnz} / \textit{n} \\ \hline
cant        & ParMetis  & 62451   & 4007383  & 64.1684 \\ \hline
consph      & PT-Scotch & 83334   & 6010480  & 72.1252 \\ \hline
memchip     & PT-Scotch & 2707524 & 13343948 & 4.9285  \\ \hline
PFlow\_742  & PT-Scotch & 742793  & 37138461 & 49.9984 \\ \hline
pkustk10    & PT-Scotch & 80676   & 4308984  & 53.4110 \\ \hline
torso3      & ParMetis  & 259156  & 4429042  & 17.0903 \\ \hline
x104        & PT-Scotch & 108384  & 8713602  & 80.3956 \\ \hline
CurlCurl\_3 & PT-Scotch & 1219574 & 13544618 & 11.1060 \\ \hline
Geo\_1438   & ParMetis  & 1437960 & 63156690 & 43.9210 \\ \hline
\end{tabular}
\caption{Assignment of SuiteSparse matrices to a specific fill-in reducing algorithm}
\label{table:SuiteSparse-ordering-assignment}
\end{table}


From now onwards, assignments mentioned in tables \ref{table:GRS-ordering-assignment}, \ref{table:SuiteSparse-ordering-assignment} is going to be used without explicitly referring to it.\\
