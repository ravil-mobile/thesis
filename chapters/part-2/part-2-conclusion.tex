\label{subseq:mm-conclusion}

In this chapter, we have examined different types of sparse linear solvers applied to linear systems generated by \acrshort{athlet} software resulting from numerical integration of thermo-hydraulic computations. We have come to the conclusion that, in spite of better scalability and parallel efficiency of iterative methods due to efficient data-based parallelism, direct sparse linear solvers are much suitable for this purpose because of its robustness.\\


In subsection \ref{subseq:mm-library-choice}, we tested different direct sparse solvers, namely: SuperLU\_DIST, PasTiX and \acrshort{mumps}. \acrshort{mumps} showed better parallel performance among the other solvers according to results of testing and, therefore, was chosen for the following study where we mainly focused on performance tuning of the library.\\


We have shown in subsequent subsections there have been four main sources of library tuning, namely:

\begin{enumerate}
	\item correct selection of a fill reducing reordering algorithm \label{conclusion:mm-1}
	\item destribution of \acrshort{mpi} processes among multiple \acrshort{numa} domain within a compute node \label{conclusion:mm-2}
	\item configuration of \acrshort{mumps} with an optimal, tuned \acrshort{blas} library implementaion \label{conclusion:mm-3}
	\item execution of \acrshort{mumps} with an optimal hybrid \acrshort{mpi}/\acrshort{openmp} processes/threads distribution \label{conclusion:mm-4}
\end{enumerate}


The testing was performed using two different matrix set, \acrshort{grs} and SuiteSparse, on two different computer-clusters, \gls{hw1} and \gls{hw2}, see chapter \ref{subseq:matrix-sets-and-hardware}, in order to check consistency of the obtained results. In this subsection, we  give most general conclusions relevant to only \acrshort{grs} matrix set and \gls{hw1} cluster as targets of the study. The reader can become familiar with detailed conclusions relative to both matrix sets and hardware at the end of each subsection that we are going to reference to bellow.\\



\ref{conclusion:mm-1}. In subsection \ref{subseq:fill-in-reordering}, it has been shown that parallel performance of \acrshort{mumps} is quite sensitive to an applied fill-in reducing reordering algorithm. A correct choice of the algorithm can lead to a significant improvement in execution time and strong scaling behavior. We have noticed that \acrshort{mumps} performs factorizations of small- and medium-sized matrices faster using PT-Scotch library whereas large-sized problems tend to get benefit from the algorithm provided by ParMetis. We assume the obtained conclusion can be not accurate due to a small size of \acrshort{grs} matrix set. At the moment of writing, we have not found any indirect method to predict a correct algorithm beforehand. Thus, we encourage \acrshort{athlet} users to perform similar testing described in the subsection before running thermo-hydraulic simulations on distributed-memory machines to achieve better performance of parallel computations.\\




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ref{conclusion:mm-2}. In subsection \ref{subseq:mm-mumps-process-pinning}, influence of different process pinning strategies on \acrshort{mumps} parallel performance has been investigated. The tests have shown that equal distribution of \acrshort{mpi} process among all available \acrshort{numa} domains always results in additional performance gain.\\ %On average, \textit{spread}-pinning allows to reduce execution time of \acrshort{mumps} by almost \textbf{5.5\%} and \textbf{13.8\%} on \gls{hw1} and \gls{hw2} machines, respectively.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ref{conclusion:mm-3}. 
In subsection \ref{subseq:blas-comparison},
we tested \acrshort{mumps} configured with 3 different implementations of \acrshort{blas} library, namely: Netlib, OpenBLAS and Intel MKL. The results have shown that application of OpenBLAS library always results in better parallel performance.\\


%Comparison and analysis of results obtained from \acrshort{grs} and SuiteSparse matrix sets allows to make an assumption that matrices generated by \acrshort{athlet} software are specific, probably due to specifics of spacial and time integration, and results in assembly trees with low number of type 2 nodes.

% we have discussed and examined a way how \acrshort{mumps} preforms partial factorizations of type 2 nodes utilizing \acrshort{blas} and \acrshort{lapack} subroutines. We have tested \acrshort{mumps} linked with 3 different implementations of BLAS: (default) Netlib, Intel MKL and OpenBLAS. The results have shown that replacement the default \acrshort{blas} implementation with a tuned one usually results in improvement of the overall solver performance. However, we have observed that degree of improvement significantly varies between test-cases. We have assumed that it may depend on the assembly tree structure of a specific test-case i.e. the ratio between type 2 and type 1 nodes.\\

%By and large, the results have shown that OpenBLAS outperforms both Netlib and Intel MKL libraries in case of \acrshort{grs} matrix set. On an average, \acrshort{mumps}-OpenBLAS configuration was about \textbf{13\%} and \textbf{21\%} faster than \acrshort{mumps}-Intel MKL and the default \acrshort{mumps}-Netlib configurations, respectively.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
\ref{conclusion:mm-4}. In subsection \ref{subseq:mpi-openmp}, we have investigated impact of various
\acrshort{mpi}/\acrshort{openmp} processes/threads distributions within a compute-node. We have observed that multi-threading of
OpenBLAS library in \acrshort{mumps} leads to multiple thread conflicts which sometimes result in significant slow-down of the solver. Results obtained with \acrshort{mumps}-Intel MKL configuration have demonstrated a negligible improvement in solver execution time resulting in a significant parallel efficiency drop, probably due to inefficient usage of additional processing elements utilized by forked Intel MKL threads. At the end, we have concluded that flat-\acrshort{mpi} mode is the best one for matrices generated by \acrshort{athlet} software.\\




%discussed how and where \acrshort{mumps} library make use of shared-memory parallelism based on review of works \cite{chowdhury2010some} and \cite{l2013introduction} \todo{problems with open blas} \todo{intel mkl}. We have also found severe slow-down of some hybrid \acrshort{mpi}/\acrshort{openmp} modes due to system and application thread conflicts  while \acrshort{mumps}-\acrshort{openmp} configuration was running on \gls{hw1} cluster. For that reason, the following study was continued with only using Intel MKL library which, in its turn, turned out to be thread-safe.\\


%We have shown that in some particular cases, factorization of \textit{Geo\_1438} matrix for example, hybrid \acrshort{mpi}-\acrshort{openmp} approach can bring significant performance improvement. However, application of hybrid computing to \acrshort{grs} matrix set gives negligible improvement on \gls{hw1} machine i.e. around \textbf{2.1\%}, and significantly deteriorates parallel efficiency. Much optimistic results were obtained for experiments conducted on \gls{hw2} machine were performance gain reached almost \textbf{31\%} for the same matrix set. \\


%During the study, we have also discovered the optimal hybrid \acrshort{mpi}/\acrshort{openmp} mode locates near the saturation point of the corresponding flat-\acrshort{mpi} test. This fact can allow to reduce the amount of testing in general. However, we do not recommend to proceed the study in this direction due to above mention parallel efficiency issues detected on \gls{hw1} machine and lack of ability to run Open\acrshort{blas} library without thread conflicts.\\


In subsection \ref{subseq:final-results}, we have studied the overall impact of introduced configuration changes found in subsections \ref{subseq:fill-in-reordering}, \ref{subseq:mm-mumps-process-pinning}, \ref{subseq:blas-comparison} and \ref{subseq:mpi-openmp}. Testing shows the changes results in a positive accumulative effect leading to considerable improvements of both factorization time and hardware utilization.\\



During the study, we have noticed that the optimal value of \acrshort{mpi} process count lays within the range between 1 and 4 in case of small-sized \acrshort{grs} matrices and 4 and 8 for middle- and large-sized problems. The exact value is impossible to predict beforehand and, therefore, it always demands individual, problem-specific testing.\\
