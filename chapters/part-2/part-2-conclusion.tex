\label{subseq:mm-conclusion}

In this chapter, we have examined different types of sparse linear solvers applied to linear systems generated by ATHLET software resulting from numerical integration of thermo-hydraulic computations. We have come to the conclusion that, in spite of better scalability and parallel efficiency of iterative methods, direct sparse linear solvers are much suitable for this purpose because of its robustness.\\


In subsection \ref{subseq:mm-library-choice}, we have tested different direct sparse solvers, namely: SuperLU\_DIST, PasTiX and MUMPS. MUMPS library showed better parallel performance among the others according to results of testing and, therefore, was chosen for the following study where we mainly focused on performance tuning of the library.\\


We have shown in subsequent subsections there have been four main sources of library tuning, namely:

\begin{enumerate}
	\item correct selection of a fill reducing reordering algorithm \label{conclusion:mm-1}
	\item destribution of MPI process among multiple NUMA domain within a compute node \label{conclusion:mm-2}
	\item configuration of MUMPS library with an optimal, tuned BLAS implementaion \label{conclusion:mm-3}
	\item execution of MUMPS with an optimal hybrid MPI/OpenMP processes/threads distribution \label{conclusion:mm-4}
\end{enumerate}


The testing was performed using two different matrix set, GRS and SuiteSparse, on two different computer-clusters, HW1 and HW2, see chapter \ref{subseq:matrix-sets-and-hardware}, to check consistency of obtained results. In this subsection, we  give most general conclusions relevant to only GRS matrix set and HW1 cluster as targets of the study. The reader can become familiar with detailed conclusions relative to both matrix sets and hardware which are presented at the end of each corresponding subsection that we are going to reference to bellow.\\


%To perform testing and analysis, two different matrix sets were used, namely: GRS and SuiteSparse. The fist one was a problem specific for thermo-hydraulic conputations whereas the second one was generated from SuiteSparse Matrix Collection \cite{sparse-matrix-collection:1}, \cite{sparse-matrix-collection:2} by downloading a dozen different matrices with respect to different numbers of equations and numbers of non-zero elements. Additionally, some tests were performed on two different compute clusters, namely: HW1 and HW2 (see chapter \ref{subseq:matrix-sets-and-hardware}), to check whether results of testing were consistent between different hardware and operating environment.\\


\ref{conclusion:mm-1}. In subsection \ref{subseq:fill-in-reordering}, it has been shown that parallel performance of MUMPS is quite sensitive to an applied fill-in reducing reordering algorithm. A correct choice of the algorithm can lead to significant improvements in execution time and strong scaling behavior. We have noticed that MUMPS performs factorizations of small- and medium-sized matrices faster using PT-Scotch library whereas large-sized problems tend to get benefit from the algorithm provided by ParMetis. We have assumed that the obtained conclusion can be not accurate due to small size of GRS matrix set. At the moment of writing  we have not found any indirect method that would predict a correct algorifactorizationthm without testing. Thus, we encourage ATHLET users perform initial testing before running thermo-hydraulic simulations.\\


 %The average performance gain reached almost \textbf{15\%} and in some particular cases it was possible to reduce execution time by approximately \textbf{40-55\%}. \todo{specify machines}\\ %During experiments, we came to the conclusion there was no an indirect metric to predict the best algorithm in advance for a specific system of equations.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ref{conclusion:mm-2}. In section \ref{subseq:mm-mumps-process-pinning}, influence of different process pinning strategies on MUMPS parallel performance has been investigated. The tests have shown that equal distribution of MPI process among all available NUMA domains always results in additional performance gain.\\ %On average, \textit{spread}-pinning allows to reduce execution time of MUMPS by almost \textbf{5.5\%} and \textbf{13.8\%} on HW1 and HW2 machines, respectively.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ref{conclusion:mm-3}. 
In section \ref{subseq:blas-comparison},
we have tested MUMPS configured with 3 different implementations of BLAS library, namely: Netlib, OpenBLAS and Intel MKL. The results have shown that application of OpenBLAS library always results in better parallel performance.\\


%Comparison and analysis of results obtained from GRS and SuiteSparse matrix sets allows to make an assumption that matrices generated by ATHLET software are specific, probably due to specifics of spacial and time integration, and results in assembly trees with low number of type 2 nodes.

% we have discussed and examined a way how MUMPS preforms partial factorizations of type 2 nodes utilizing BLAS and LAPACK subroutines. We have tested MUMPS linked with 3 different implementations of BLAS: (default) Netlib, Intel MKL and OpenBLAS. The results have shown that replacement the default BLAS implementation with a tuned one usually results in improvement of the overall solver performance. However, we have observed that degree of improvement significantly varies between test-cases. We have assumed that it may depend on the assembly tree structure of a specific test-case i.e. the ratio between type 2 and type 1 nodes.\\

%By and large, the results have shown that OpenBLAS outperforms both Netlib and Intel MKL libraries in case of GRS matrix set. On an average, MUMPS-OpenBLAS configuration was about \textbf{13\%} and \textbf{21\%} faster than MUMPS-Intel MKL and the default MUMPS-Netlib configurations, respectively.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{subseq:mpi-openmp}
\ref{conclusion:mm-4}. In section \ref{subseq:mpi-openmp}, we have investigated impact of various MPI/OpenMP processes/threads distribution within a compute-node. We have observed that multi-threading of OpenBLAS library in MUMPS leads to multiple thread conflicts which sometimes result in significant slow-down of the solver. Results obtained with MUMPS-Intel MKL configuration have demonstrated negligible improvements in solver execution time and significant drop of parallel efficiency, probably due to inefficient usage of additional processing elements utilized by Intel MKL. At the end, we have come to the conclusion that flat-MPI mode is the best one for matrices generated by ATHLET software.\\




%discussed how and where MUMPS library make use of shared-memory parallelism based on review of works \cite{chowdhury2010some} and \cite{l2013introduction} \todo{problems with open blas} \todo{intel mkl}. We have also found severe slow-down of some hybrid MPI/OpenMP modes due to system and application thread conflicts  while MUMPS-OpenMP configuration was running on HW1 cluster. For that reason, the following study was continued with only using Intel MKL library which, in its turn, turned out to be thread-safe.\\


%We have shown that in some particular cases, factorization of \textit{Geo\_1438} matrix for example, hybrid MPI-OpenMP approach can bring significant performance improvement. However, application of hybrid computing to GRS matrix set gives negligible improvement on HW1 machine i.e. around \textbf{2.1\%}, and significantly deteriorates parallel efficiency. Much optimistic results were obtained for experiments conducted on HW2 machine were performance gain reached almost \textbf{31\%} for the same matrix set. \\


%During the study, we have also discovered the optimal hybrid MPI/OpenMP mode locates near the saturation point of the corresponding flat-MPI test. This fact can allow to reduce the amount of testing in general. However, we do not recommend to proceed the study in this direction due to above mention parallel efficiency issues detected on HW1 machine and lack of ability to run OpenBLAS library without thread conflicts.\\


Finally, in subsection \ref{}, we have compared the overall impact of applied configuration parameters. According to results of the final testing, ...\\
