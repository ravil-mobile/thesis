\chapter{Problem Statement}\label{chapter:problem-statment}


Integration of a system of \acrshort{ode}s by means of W-methods involves solving  several systems of linear equations. Equations \ref{eq:athlet-9} can be rewritten in a form \ref{eq:athlet-10}, after grouping both the right- and left-hand sides in a single matrix and vector, respectively.\\


%\begin{equation} \label{eq:athlet-10}
%	A_{i} \Delta z^{l}_{i} =  b^{l}_{i} 
%\end{equation}

\begin{equation} \label{eq:athlet-10}
	A_{i} \delta y_{ij} =  b_{ij} 
\end{equation}


where $A_{i} = (I - h_{i}J)$  is a $n \times n$ nonsingular sparse matrix; $\delta y_{ij}$  and $b_{ij}$ are $\mathbb{R}^{n}$ vectors.\\


According to the integration scheme, Figure \ref{fig:introduction-w-method-scheme}, and definition of the method, each step of numerical integration requires to solve 6 linear systems with 3 distinct matrices, resulting from the Jacobian matrix by the corresponding shifts of the main diagonal. Therefore, the computational burden of the W-method mainly lies in both solving sparse linear systems and evaluations of non-linear function $f$, Equation \ref{eq:athlet-8}. However, because of the time limit of the thesis, the main focus of this study is on solving Systems \ref{eq:athlet-10} efficiently on \acrshort{grs} computational cluster.\\


There exist two families of linear sparse solvers, namely: iterative and direct sparse methods. In the general case, execution time of any method, regardless of a solver family, is bounded by $O(n^2)$ complexity due to matrix sparsity, where $n$ is the number of equations in a system. However, constants in front of the factor $n^2$ can vary significantly between the methods which explains differences in execution time. Additionally, it is important to mention that the families follow different approaches for solving sparse linear systems and are greatly different in details. Therefore, they possess different numerical properties. Among all  properties, there are some which are particularly important for efficient execution of W-methods, namely: \\


\begin{itemize}
	\item robustness of a method to treat, in particular, ill-conditioned systems
	\item parallel efficiency
\end{itemize}


These, above mentioned properties, can be treated as non-functional requirements to a sparse linear solver for efficient numerical time integration.\\

 
Finding solutions of sparse linear systems is a well-known and commonly occurring problem in the field of scientific computing and, therefore, numerous implementations of different kinds of linear solvers exist. However, the \acrshort{nut} project imposes some extra constrains due to the design philosophy adopted by \acrshort{grs}: \\


\begin{itemize}
	\item open-source license
	\item direct interface to \acrshort{petsc}
	\item technical support and maintainability of a solver/package
\end{itemize}

%In particular, it requires a particular solver implementation to have its \textbf{open-source license} and to have its direct \textbf{interface to \acrshort{petsc}} library.\\


In this study, we are primarily concerned with a selection and configuration of a  sparse linear solver which can cover all above listed requirements.\\


This report is organized as follows. Chapter \ref{subseq:matrix-sets-and-hardware} provides information about methodology, data, software and hardware used in this study. Subsections \ref{subseq:part-2-iterative-solvers-theory} and \ref{subseq:part-2-direct-sparse-solvers-theory} give an overview of the theory and parallelization aspects of iterative and sparse direct methods as well as discussions of some issues related to numerical solution accuracy. Then, in Subsection \ref{subseq:hybrid-method-description}, we make a conclusion about which type of sparse linear solvers is well suited for numerical time integration governed by W-methods. In Section \ref{subseq:mm-library-choice}, a concrete implementation of a specific method is selected by means of testing. From Section \ref{mumps:solver-configuration} onwards, we perform configuration and adaptation of a solver for distributed-memory computations. At the end, Section \ref{subseq:mm-conclusion} summarizes obtained results and makes a general conclusion with respect to data and compute environment provided by  \acrshort{grs}.\\


An additional topic, considered in this study, is an improvement of \acrshort{athlet}-\acrshort{nut} communication during Jacobian matrix transfers. As it was described in Section \ref{sec:athlet-nut-coupling}, \acrshort{athlet}, the client, transfers a Jacobian matrix in a column-wise fashion. \acrshort{nut}, the server, treats each column transfer as a service and, therefore, each transfer passes through the 3-way handshake described in Section \ref{sec:athlet-nut-coupling}. Moreover, it is important to mention one more time,   due to the current implementation of \acrshort{athlet}-\acrshort{nut} coupling, client-server communication is blocking. In other words, \acrshort{athlet} gets blocked till completion of a column transfer. \\


The main goal of Jacobian matrix compression, described in Section \ref{sec:jacobian-matrix-compression}, is to minimize the number of perturbations of non-linear function $f$ of Equation \ref{eq:athlet-8}. Additionally it allows to reduce an amount of column transfers as well. Therefore, it improves the overall application performance from both computational and communication points of view. However, there are still some aspects to be considered.\\


Due to specifics of a matrix compression algorithm, described in Section \ref{sec:jacobian-matrix-compression}, column lengths are decreasing between the first and last columns of a compressed Jacobian matrix form which, as a result, leads to unequal \acrshort{mpi} message sizes.\\


In the last part of the study, we introduce a concept called \textit{accumulator} which allows to transfer a compressed Jacobian matrix in equal chunks. This approach potentially solves three important problems at once. First of all, \textit{accumulator} can help to get rid of small \acrshort{mpi} messages which improves utilization of network bandwidth. Secondly, it helps to reduce an amount of synchronizations between a client and the server and, therefore, improves operation of \acrshort{nut} as the server. Lastly, it allows to apply non-blocking \acrshort{mpi} communication on the client side and thus overlap Jacobian matrix transfers with computations.\\


In Section \ref{sec:jacobian-matrix-compression}, we briefly describe the Jacobian matrix compression algorithm and the resulting \acrshort{athlet}-\acrshort{nut} communication problem. In Section \ref{sec:accumulator-approach}, we present and describe an algorithm which is supposed to resolve the problem. Section \ref{sec:benchmark-and-test-data} provides a description of developed benchmarks and test data. Then, we discuss obtained results in Section \ref{sec:accumulator-results}. Finally, in Section \ref{sec:accumulator-conclusions}, we provide a general conclusion of the performed part of the study and summarize the results.\\
